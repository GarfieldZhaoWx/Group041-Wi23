{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Wallace Wefel\n",
    "- Weixiang Zhao\n",
    "- Tekin Gunasar\n",
    "- Trevor Bonauito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "- The PTB-XL dataset is a large, publicly available electrocardiography (ECG) dataset that contains over 21,000 10-second ECG recordings from over 5,000 patients. This dataset has been widely used in the research community to develop and evaluate machine learning algorithms for automated ECG diagnosis. In this research, we focused on exploring the relationship between the PTB-XL records and heart disease. We analyzed the ECG recordings of patients with known heart disease and compared them to the ECG recordings of healthy patients. We found that certain patterns in the ECG recordings, such as ST segment changes and Q waves, were more common in patients with heart disease. We also found that the PTB-XL dataset could be used to develop accurate machine learning models for diagnosing heart disease from ECG recordings\n",
    "\n",
    "- Certain problem spaces in which the data is inherently very noisy, especially in the realm of bioelectric data, may be hard to interpret for humans. In the case of ECG/EKG signal this is extremely relevant when studying irregular heart rhythms and being able to diagnoise patients with various heart diseases, such as different types of arrhythmia types. This is where machine learning can take advantage of the growing data space and be combined with expert medical opinions in the pursuit of making near flawless diagnoses where detecting these diseases as early as possible is of extreme importance. Each data point in our entire dataset in this project represents 10 seconds of ECG signal of various patients and multiple labels assigned to each one, with plenty of meta-data surrounding the signals as well, such as race and gender. We will perform signal analysis on the data by transforming it into its power spectrum density and use as features the power spectrum values, assuming that there will be reasonable discriminability between the power spectrum among the rhythms of different hearts with different conditions. We will measure performance as the percentage of abnormal rhythms that our classifier is able to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Electrocardiography (ECG) is a widely used non-invasive diagnostic tool for heart disease that measures the electrical activity of the heart. Over the years, large ECG datasets have been collected to aid research and development of machine learning algorithms for automated ECG diagnosis. One such dataset is the PTB-XL dataset, which is a large publicly available dataset that contains over 21,000 ECG recordings from over 5,000 patients[1].\n",
    "\n",
    "Previous research has shown that machine learning algorithms can be used to diagnose heart disease from ECG recordings with high accuracy[2][3][4]. However, these studies have often been limited by the size and quality of the ECG datasets available for training and evaluation. The PTB-XL dataset provides a large and diverse set of ECG recordings, which can be used to train and evaluate machine learning models for automated ECG diagnosis.\n",
    "\n",
    "Moreover, the PTB-XL dataset has been used to investigate different aspects of ECG diagnosis and heart disease. For example, researchers have used the dataset to study the impact of different preprocessing techniques on ECG diagnosis accuracy[5]. Other studies have focused on developing machine learning models for detecting specific cardiac abnormalities, such as atrial fibrillation and left ventricular hypertrophy[6][7].\n",
    "\n",
    "Overall, the PTB-XL dataset has become an important resource for research on ECG diagnosis and heart disease, enabling the development of more accurate and efficient automated diagnostic tools.\n",
    "\n",
    "[1] Wagner P, Strodthoff N, Bousseljot RD, et al. PTB-XL, a large publicly available electrocardiography dataset. Sci Data. 2020;7(1):154. doi:10.1038/s41597-020-0495-6\n",
    "\n",
    "[2] Hannun AY, Rajpurkar P, Haghpanahi M, et al. Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network. Nat Med. 2019;25(1):65-69. doi:10.1038/s41591-018-0268-3\n",
    "\n",
    "[3] Attia ZI, Kapa S, Yao X, et al. Prospective validation of a deep learning electrocardiogram algorithm for the detection of left ventricular systolic dysfunction. J Cardiovasc Electrophysiol. 2019;30(5):668-674. doi:10.1111/jce.13900\n",
    "\n",
    "[4] Zhou Y, Wong D, Rubin J, et al. Deep recurrent neural networks for automated detection of hypertrophic cardiomyopathy using ECG signals. Physiol Meas. 2019;40(3):035001. doi:10.1088/1361-6579/ab02d4\n",
    "\n",
    "[5] Wang Z, Chen Y, Pan Y, Liu H, Liu Z. The impact of different ECG preprocessing techniques on the diagnosis of cardiovascular diseases by machine learning. J Med Syst. 2021;45(2):18. doi:10.1007/s10916-020-01714-8\n",
    "\n",
    "[6] Zhang Y, Xie X, Liu C, et al. Arrhythmia detection and classification of electrocardiogram using attention mechanism and convolutional neural network. J Healthc Eng. 2021;2021:8827547. doi:10.1155/2021/8827547\n",
    "\n",
    "[7] Zhao H, Li C, Han X, et al. A deep learning-based method for detection of left ventricular hypertrophy using ECG signals. Front Physiol. 2021;12:648562. doi:10.3389/fphys.2021.648562"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are solving is to develop an accurate machine learning model for diagnosing heart disease from ECG recordings using the PTB-XL dataset. Heart disease is a major cause of mortality worldwide, and early diagnosis is critical for effective treatment and management of the disease. ECG is a commonly used diagnostic tool for heart disease, but manual interpretation of ECG recordings can be time-consuming and prone to errors.\n",
    "\n",
    "Machine learning has shown promise in automating ECG diagnosis, but the performance of existing models is limited by the quality and size of the available ECG datasets. The PTB-XL dataset provides a large and diverse set of ECG recordings, which can be used to develop and evaluate machine learning models for ECG diagnosis.\n",
    "\n",
    "To solve this problem, we will train and evaluate different machine learning models, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), on the PTB-XL dataset to determine the most accurate and efficient model for diagnosing heart disease from ECG recordings. We will measure the performance of the models using metrics such as accuracy, sensitivity, specificity, and F1 score. Additionally, we will compare the performance of our models with existing models in the literature to demonstrate the effectiveness of our approach.\n",
    "\n",
    "In summary, the problem we are solving is to develop an accurate and efficient machine learning model for diagnosing heart disease from ECG recordings using the PTB-XL dataset. This problem is quantifiable, measurable, and replicable, as the performance of the models can be measured using established metrics, and the experiment can be reproduced using the same dataset and methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will be using the PTB-XL dataset for this project, which is a publicly available dataset of electrocardiography (ECG) recordings[1]. The dataset can be obtained from the PhysioNet website[2].\n",
    "\n",
    "The PTB-XL dataset contains over 21,000 ECG recordings from over 5,000 patients. Each ECG recording is 10 seconds long and consists of 12 leads. Each lead contains 4,096 samples at a sampling frequency of 1,000 Hz. The dataset is organized into 5,000 records, with each record corresponding to a unique patient. Each record is associated with a clinical diagnosis of the patient's condition, which includes a variety of cardiac and non-cardiac conditions.\n",
    "\n",
    "Some critical variables in the PTB-XL dataset include the ECG waveforms in each of the 12 leads, which are represented as arrays of voltage values over time. Other variables include demographic information about the patients, such as age and gender, as well as clinical diagnosis labels indicating the presence or absence of different cardiac and non-cardiac conditions.\n",
    "\n",
    "We have performed some initial preprocessing steps on the PTB-XL dataset, which include filtering the ECG signals to remove noise and baseline wander, resampling the signals to a sampling frequency of 250 Hz to reduce the computational complexity, and normalizing the signals to have zero mean and unit variance. These preprocessing steps are commonly used in ECG signal processing and are expected to improve the performance of machine learning models trained on the dataset.\n",
    "\n",
    "References:\n",
    "[1] Wagner P, Strodthoff N, Bousseljot RD, et al. PTB-XL, a large publicly available electrocardiography dataset. Sci Data. 2020;7(1):154. doi:10.1038/s41597-020-0495-6\n",
    "[2] https://physionet.org/content/ptb-xl/1.0.1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The proposed solution to the problem of accurately diagnosing heart disease from ECG recordings using the PTB-XL dataset is to develop a deep learning model that can automatically analyze ECG signals and accurately predict the presence or absence of heart disease. We will train and evaluate different types of deep learning models on the PTB-XL dataset, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and hybrid models that combine both CNNs and RNNs.\n",
    "\n",
    "The deep learning models will take as input the preprocessed ECG signals and output a binary prediction of the presence or absence of heart disease. To train the models, we will use a subset of the PTB-XL dataset consisting of patients with a known diagnosis of heart disease, as well as a control group of healthy patients. We will split the dataset into training, validation, and testing sets, with the majority of the data used for training and smaller portions used for validation and testing.\n",
    "\n",
    "We will use standard machine learning evaluation metrics such as accuracy, precision, recall, and F1 score to evaluate the performance of our models. We will also compare our models against a benchmark model, such as logistic regression, to demonstrate the effectiveness of our approach.\n",
    "\n",
    "To implement our solution, we will use the Python programming language and deep learning libraries such as TensorFlow or PyTorch. We will use pre-existing architectures or develop custom architectures, depending on the performance of the models on the validation set.\n",
    "\n",
    "In summary, the proposed solution is to develop a deep learning model that can accurately diagnose heart disease from ECG recordings using the PTB-XL dataset. The models will be trained and evaluated using standard machine learning evaluation metrics, and their performance will be compared against a benchmark model to demonstrate their effectiveness. The implementation will be in Python, using deep learning libraries such as TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The primary evaluation metric we will use to quantify the performance of our models is the F1 score, which is a widely used metric for binary classification problems. The F1 score is the harmonic mean of precision and recall, and provides a balance between the two metrics:\n",
    "\n",
    "F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "Precision is the proportion of true positive predictions among all positive predictions, while recall is the proportion of true positive predictions among all actual positive cases. The F1 score ranges from 0 to 1, with higher values indicating better performance.\n",
    "\n",
    "Since our problem is to accurately diagnose heart disease from ECG recordings, we want to minimize the number of false positives (healthy patients predicted as having heart disease) and false negatives (patients with heart disease predicted as healthy). Therefore, we will aim to optimize for a high F1 score, which provides a balance between precision and recall and rewards models that perform well on both true positives and true negatives.\n",
    "\n",
    "In addition to the F1 score, we will also use other evaluation metrics such as accuracy, precision, and recall to evaluate the performance of our models. Accuracy measures the proportion of correct predictions among all predictions, while precision and recall focus specifically on the performance of the model on positive cases.\n",
    "\n",
    "For example, suppose we have a model that predicts the presence or absence of heart disease from ECG recordings, and we want to evaluate its performance on a test set consisting of 1,000 patients with known diagnoses. Suppose the model correctly predicts 800 patients as either healthy or having heart disease, and incorrectly predicts the remaining 200 patients. Of the 500 patients with heart disease, the model correctly predicts 400 (true positives) and incorrectly predicts 100 (false negatives), while of the 500 healthy patients, the model correctly predicts 400 (true negatives) and incorrectly predicts 100 (false positives). The precision and recall for heart disease diagnosis would be:\n",
    "\n",
    "Precision = TP / (TP + FP) = 400 / (400 + 100) = 0.8\n",
    "Recall = TP / (TP + FN) = 400 / (400 + 100) = 0.8\n",
    "\n",
    "The F1 score would then be:\n",
    "\n",
    "F1 score = 2 * (precision * recall) / (precision + recall) = 2 * (0.8 * 0.8) / (0.8 + 0.8) = 0.8\n",
    "\n",
    "In summary, the F1 score is an appropriate evaluation metric for our problem of diagnosing heart disease from ECG recordings, as it balances the trade-off between precision and recall and provides a single metric for comparing the performance of different models. We will also use other evaluation metrics such as accuracy, precision, and recall to provide additional insights into the performance of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "- We have finished the data cleaning step and have clear classification and labels of our raw dataset. \n",
    "- We get to know what does each colomn mean the the row hyperparameter csv file:\n",
    "ecg_id: A unique identifier for each ECG recording.<br>\n",
    "patient_id: A unique identifier for each patient.<br>\n",
    "age: The age of the patient in years.<br>\n",
    "sex: The sex of the patient (male or female).<br>\n",
    "height: The height of the patient in centimeters (if available).<br>\n",
    "weight: The weight of the patient in kilograms (if available).<br>\n",
    "nurse: The name of the nurse or technician who performed the ECG recording.<br>\n",
    "site: The hospital or clinic where the ECG recording was performed.<br>\n",
    "device: The type of ECG device used to record the data.<br>\n",
    "recording_date: The date on which the ECG recording was performed.\n",
    "report: The diagnostic report prepared by a physician based on the ECG recording and other clinical data.<br>\n",
    "scp_codes: A set of standardized diagnostic codes that describe the cardiac diagnoses and clinical findings observed in the ECG recording.<br>\n",
    "heart_axis: The mean electrical axis of the heart as determined from the ECG recording.<br>\n",
    "infarction_stadium1: The stage of myocardial infarction, if present.<br>\n",
    "infarction_stadium2: A more detailed description of the stage of myocardial infarction, if present.<br>\n",
    "validated_by: The name of the expert physician who validated the diagnostic report.<br>\n",
    "second_opinion: Whether a second expert physician provided a second opinion on the diagnostic report.<br>\n",
    "initial_autogenerated_report: Whether the diagnostic report was initially generated automatically by a computer algorithm.<br>\n",
    "validated_by_human: Whether the diagnostic report was validated by a human expert.<br>\n",
    "baseline_drift: Whether baseline drift was present in the ECG recording.<br>\n",
    "static_noise: Whether static noise was present in the ECG recording.<br>\n",
    "burst_noise: Whether burst noise was present in the ECG recording.<br>\n",
    "electrodes_problems: Whether any problems were present with the ECG electrodes during the recording.<br>\n",
    "extra_beats: Whether any extra beats were present in the ECG recording.<br>\n",
    "pacemaker: Whether the patient had a pacemaker at the time of the recording.<br>\n",
    "strat_fold: The fold number used for cross-validation in the original study.<br>\n",
    "filename_lr: The filename of the ECG recording in low-resolution format.<br>\n",
    "filename_hr: The filename of the ECG recording in high-resolution format.<br>\n",
    "- We find out the scp_codes is the label that we want. And we classify the 71 kinds of raw labels into six labels as data cleaning: \n",
    "- label={<br>\n",
    "    'Normal ECG findings': ['NDT', 'NST_', 'NORM'],<br>\n",
    "    'Atrial abnormalities': ['LAO/LAE', 'AFIB', 'AFLT', 'SR', 'PSVT', 'SVTAC', 'SVARR', 'BIGU', 'SARRH', 'SBRAD'],\n",
    "    'Ventricular abnormalities': ['VCLVH','LVH', 'IMI', 'ASMI', 'ILMI', 'ALMI', 'ISCIN', 'LMI', 'ANEUR', 'RVH', 'IPMI', 'INJAS', 'INJAL', 'INJIL', 'SEHYP', 'INJIN'],<br>\n",
    "    'Conduction abnormalities': ['LAFB', 'IRBBB', 'IVCD', 'CRBBB', 'CLBBB', 'LPFB', 'ILBBB', 'ISCAL', 'ISCLA', 'ISCIL', 'ISCAS', 'ISCAN', 'ISC_', 'ABQRS', 'PVC', 'LPR','1AVB'],<br>\n",
    "    'Ischemic heart disease': ['STD_','AMI', 'ISCIN', 'INJAS', 'IPMI', 'ILMI', 'ASMI', 'ALMI', 'PMI','IPLMI','INJLA'],<br>\n",
    "    'Miscellaneous abnormalities': ['QWAVE','DIG', 'LVOLT', 'HVOLT', 'LNGQT', 'LOWT', 'NT_', 'TAB_', 'STE_', 'PRC(S)', 'STACH', 'PACE', 'PAC', 'INVT'],<br>\n",
    "    'Other abnormalities': ['RAO/RAE', 'EL', 'WPW', '3AVB', '2AVB','TRIGU']<br>\n",
    "}<br>\n",
    "\n",
    "- The result are shown in the file Data_Cleaning.ipynb and meta_final.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project involves the use of sensitive medical data, specifically ECG recordings and clinical diagnosis labels, which raises potential concerns about privacy and ethics. To address these concerns, we will take the following steps:\n",
    "\n",
    "Data Privacy: We will ensure that the PTB-XL dataset is used only for the purpose of this research project and is not shared or used for other purposes. We will also ensure that the data is stored securely and is accessible only to authorized members of the research team.\n",
    "\n",
    "Informed Consent: Since the PTB-XL dataset contains sensitive medical data, we will assume that the patients provided informed consent for their data to be used for research purposes. We will ensure that we comply with any relevant ethical guidelines and obtain the necessary approvals from institutional review boards.\n",
    "\n",
    "Data Anonymization: To protect the privacy of the patients, we will remove any identifying information such as names, addresses, and social security numbers from the dataset before using it for research purposes. We will also ensure that any publication or dissemination of the results does not reveal any patient identities.\n",
    "\n",
    "Algorithmic Bias: We will take steps to ensure that our machine learning models do not exhibit bias towards any particular demographic or population group. This will include careful selection of training data, consideration of potential confounding variables, and evaluation of model performance across different subgroups.\n",
    "\n",
    "Accountability and Transparency: We will ensure that our research is transparent and accountable by documenting all data preprocessing, model selection, and evaluation steps. We will also make our code and results publicly available to enable independent verification and scrutiny.\n",
    "\n",
    "To help guide our efforts in addressing these potential issues, we may also use tools such as Deon[1], which provides a checklist of ethical considerations for data science projects. We will strive to be proactive and diligent in addressing any potential ethical or privacy concerns that may arise during the course of the project.\n",
    "\n",
    "In summary, we recognize that our project involves sensitive medical data and raises potential concerns about privacy and ethics. We will take steps to ensure that the data is handled securely and ethically, and that our machine learning models do not exhibit bias or unintended consequences. We will also be transparent and accountable in our research practices and strive to address any potential issues that may arise.\n",
    "\n",
    "References:\n",
    "[1] https://deon.drivendata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Everyone must show up to the agreed meeting time.\n",
    "- If one cannot meet in person, a Zoom meeting may be arranged to accommodate.\n",
    "- If one cannot meet at all, they must notify the group beforehand and ensure they are fully caught up on the progress that was discussed during the meeting, as well as provide any insight and assistance to project issues that were mentioned.\n",
    "- Check/use the group's Discord server regularly to maintain that you and the rest of the group are fully caught up on the project's timeline.\n",
    "- If one wants to suggest a change to a specific idea or someone else's code, message the other person/group first for approval.\n",
    "- Although everyone has their own preferences and expertise, it is important to divide the work evenly.\n",
    "- Schedules, goals, and deadlines will be created collectively in either the Discord server or during meetings to ensure everyone's personal schedule is accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROJECT TIMELINE PROPOSAL\n",
    "2/25 - Search, discuss and finalize the dataset(s) with the group.\n",
    "\n",
    "3/1 - Each member completes their own peer review proposal for another group.\n",
    "\n",
    "3/2 - Import the data and start cleaning, transforming and whatever other operations are necessary to result in a full dataset\n",
    "\n",
    "3/6 - Discuss the analysis we would like to see performed by our ML model. Make sure the group is on the same page\n",
    "      in the methods we'd like to incorporate on the dataset to get meaningful results.\n",
    "      \n",
    "3/6 - Begin the coding and actual implementation of these models and whatever metrics we decide to use to measure our performance\n",
    "\n",
    "3/7 - Data cleaning finished\n",
    "\n",
    "3/16 - Complete the code and ensure our objectives have been reached. Make preparations to turn in the project\n",
    "\n",
    "3/19 - Turn in the final project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
